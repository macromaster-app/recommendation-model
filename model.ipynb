{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\janki\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janki\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Text\n",
    "import pprintpp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from tensorflow import keras # type: ignore\n",
    "from keras import layers, Sequential, Model, losses, metrics, optimizers, backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_data = pd.read_csv(\"./data/RAW_interactions.csv\")\n",
    "recipe_data = pd.read_csv(\"./data/RAW_recipes.csv\")\n",
    "\n",
    "interaction_train = pd.read_csv(\"./data/interactions_train.csv\")\n",
    "interaction_test = pd.read_csv(\"./data/interactions_test.csv\")\n",
    "interaction_validation = pd.read_csv(\"./data/interactions_validation.csv\")\n",
    "\n",
    "single_user_test = pd.read_csv(\"./data/single_user_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231637, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2886</td>\n",
       "      <td>9.601258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27208</td>\n",
       "      <td>9.697352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89204</td>\n",
       "      <td>9.617157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39087</td>\n",
       "      <td>9.862974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67256</td>\n",
       "      <td>9.571510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   recipe_id     score\n",
       "0       2886  9.601258\n",
       "1      27208  9.697352\n",
       "2      89204  9.617157\n",
       "3      39087  9.862974\n",
       "4      67256  9.571510"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get avg ratings for each recipe and sort by number of ratings\n",
    "df3_avg = interaction_data.groupby('recipe_id')\n",
    "df3_avg = df3_avg['rating'].agg(['mean', 'count']).sort_values(by='count', ascending=False)\n",
    "#show top 10s names from df2 and ratings from df3_avg\n",
    "df3_avg = df3_avg.merge(recipe_data, left_on='recipe_id', right_on='id')\n",
    "# change id to recipe_id\n",
    "df3_avg = df3_avg.rename(columns={'id': 'recipe_id'})\n",
    "# drop unnecessary columns\n",
    "df3_avg = df3_avg.drop(columns=['minutes', 'contributor_id', 'n_steps', 'n_ingredients'])\n",
    "df4 = df3_avg[:]\n",
    "\n",
    "def score(mean, count):\n",
    "    return mean + 2 * np.log10(count) -1\n",
    "\n",
    "df3_avg = df3_avg[['recipe_id', 'mean', 'count']]\n",
    "# merge columns mean and count to get score\n",
    "df3_avg['score'] = df3_avg.apply(lambda x: score(x['mean'], x['count']), axis=1)\n",
    "df3_avg = df3_avg.drop(columns=['mean', 'count'])\n",
    "\n",
    "# merge interaction train and test with df3_avg\n",
    "interaction_data = interaction_data.merge(df3_avg, left_on='recipe_id', right_on='recipe_id')\n",
    "interaction_train = interaction_train.merge(df3_avg, left_on='recipe_id', right_on='recipe_id')\n",
    "interaction_test = interaction_test.merge(df3_avg, left_on='recipe_id', right_on='recipe_id')\n",
    "interaction_validation = interaction_validation.merge(df3_avg, left_on='recipe_id', right_on='recipe_id')\n",
    "\n",
    "# merge df4 with df3_avg\n",
    "df4 = df4.merge(df3_avg, left_on='recipe_id', right_on='recipe_id')\n",
    "    \n",
    "\n",
    "\n",
    "print(df3_avg.shape)\n",
    "df3_avg.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_data = interaction_data.astype({'user_id': 'string', 'recipe_id':'string', 'rating':'float64', \"score\":'float64'})\n",
    "interaction_train = interaction_train.astype({'user_id': 'string', 'recipe_id':'string', 'rating':'float64', \"score\":'float64'})\n",
    "interaction_test = interaction_test.astype({'user_id': 'string', 'recipe_id':'string', 'rating':'float64', \"score\":'float64'})\n",
    "interaction_validation = interaction_validation.astype({'user_id': 'string', 'recipe_id':'string', 'rating':'float64', \"score\":'float64'})\n",
    "\n",
    "single_user_test = single_user_test.astype({'user_id': 'string', 'recipe_id':'string', 'rating':'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueUserIds = interaction_data.user_id.unique()\n",
    "uniqueFoodIds = interaction_data.recipe_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        self.user_embeddings = Sequential([\n",
    "                                    layers.experimental.preprocessing.StringLookup(\n",
    "                                        vocabulary=uniqueUserIds, mask_token=None),\n",
    "                                    layers.Embedding(len(uniqueUserIds)+1, embedding_dimension)\n",
    "                                    ])\n",
    "\n",
    "        self.food_embeddings = Sequential([\n",
    "                                    layers.experimental.preprocessing.StringLookup(\n",
    "                                        vocabulary=uniqueFoodIds, mask_token=None),\n",
    "                                    layers.Embedding(len(uniqueFoodIds)+1, embedding_dimension),\n",
    "                                    ])\n",
    "        self.ratings = Sequential([\n",
    "                            layers.Dense(256, activation=\"relu\"),\n",
    "                            layers.Dense(64,  activation=\"relu\"),\n",
    "                            layers.Dense(16,  activation=\"relu\"),\n",
    "                            layers.Dense(1)\n",
    "                              ])\n",
    "\n",
    "        \n",
    "    def call(self, userId, foodId):\n",
    "        user_embeddings  = self.user_embeddings (userId)\n",
    "        food_embeddings = self.food_embeddings(foodId)\n",
    "        return self.ratings(tf.concat([user_embeddings, food_embeddings], axis=1))\n",
    "\n",
    "class FoodModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ranking_model: Model = RankingModel()\n",
    "        self.task: layers.Layer   = tfrs.tasks.Ranking(\n",
    "                                                    loss    =  losses.MeanSquaredError(),\n",
    "                                                    metrics = [metrics.RootMeanSquaredError()])\n",
    "        self.validation_metrics = [metrics.RootMeanSquaredError()]\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        rating_predictions = self.ranking_model(features[\"userID\"], features[\"foodID\"]  )\n",
    "\n",
    "        return self.task( labels=features[\"rating\"], predictions=rating_predictions)\n",
    "    \n",
    "\n",
    "    def save_model(self, *args, **kwargs):\n",
    "        self.ranking_model.save(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score formula for ranking = avg rating + math.log(number of ratings)\n",
    "def score_formula(avg_rating, num_ratings):\n",
    "    return avg_rating + np.log(num_ratings)\n",
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "{\n",
    "    \"userID\":tf.cast(interaction_train.user_id.values, tf.string),\n",
    "    \"foodID\":tf.cast(interaction_train.recipe_id.values, tf.string),\n",
    "    \"rating\":tf.cast(interaction_train.rating.values, tf.float32),\n",
    "    \"score\":tf.cast(interaction_train.score.values, tf.float32)\n",
    "})\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "{\n",
    "    \"userID\":tf.cast(interaction_test.user_id.values, tf.string),\n",
    "    \"foodID\":tf.cast(interaction_test.recipe_id.values, tf.string),\n",
    "    \"rating\":tf.cast(interaction_test.rating.values, tf.float32),\n",
    "    \"score\":tf.cast(interaction_test.score.values, tf.float32)\n",
    "})\n",
    "\n",
    "validation_data = tf.data.Dataset.from_tensor_slices(\n",
    "{\n",
    "    \"userID\":tf.cast(interaction_validation.user_id.values, tf.string),\n",
    "    \"foodID\":tf.cast(interaction_validation.recipe_id.values, tf.string),\n",
    "    \"rating\":tf.cast(interaction_validation.rating.values, tf.float32),\n",
    "    \"score\":tf.cast(interaction_validation.score.values, tf.float32)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = train_data.shuffle(100_000, seed=12, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "86/86 [==============================] - 10s 96ms/step - root_mean_squared_error: 2.6674 - loss: 6.9936 - regularization_loss: 0.0000e+00 - total_loss: 6.9936 - val_root_mean_squared_error: 1.3244 - val_loss: 2.0249 - val_regularization_loss: 0.0000e+00 - val_total_loss: 2.0249\n",
      "Epoch 2/5\n",
      "86/86 [==============================] - 5s 56ms/step - root_mean_squared_error: 0.8858 - loss: 0.7806 - regularization_loss: 0.0000e+00 - total_loss: 0.7806 - val_root_mean_squared_error: 1.2800 - val_loss: 1.8432 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.8432\n",
      "Epoch 3/5\n",
      "86/86 [==============================] - 5s 57ms/step - root_mean_squared_error: 0.8333 - loss: 0.6894 - regularization_loss: 0.0000e+00 - total_loss: 0.6894 - val_root_mean_squared_error: 1.2945 - val_loss: 1.8713 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.8713\n",
      "Epoch 4/5\n",
      "86/86 [==============================] - 5s 56ms/step - root_mean_squared_error: 0.8133 - loss: 0.6561 - regularization_loss: 0.0000e+00 - total_loss: 0.6561 - val_root_mean_squared_error: 1.3100 - val_loss: 1.9094 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.9094\n",
      "Epoch 5/5\n",
      "86/86 [==============================] - 5s 59ms/step - root_mean_squared_error: 0.8040 - loss: 0.6408 - regularization_loss: 0.0000e+00 - total_loss: 0.6408 - val_root_mean_squared_error: 1.3196 - val_loss: 1.9314 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.9314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19f79dbdd90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FoodModel()\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False),\n",
    "                metrics=model.validation_metrics)\n",
    "# clear cache from previous runs\n",
    "backend.clear_session()\n",
    "\n",
    "cached_train = train_data.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test_data.batch(4096).cache()\n",
    "cached_validation = validation_data.batch(4096).cache()\n",
    "model.fit(cached_train, epochs=5, validation_data=cached_validation, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 [======>.......................] - ETA: 0s - root_mean_squared_error: 1.2884 - loss: 1.6599 - regularization_loss: 0.0000e+00 - total_loss: 1.6599"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 9ms/step - root_mean_squared_error: 1.3715 - loss: 2.1699 - regularization_loss: 0.0000e+00 - total_loss: 2.1699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'root_mean_squared_error': 1.3714897632598877,\n",
       " 'loss': 2.618216037750244,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 2.618216037750244}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408066\n",
      "test_rating\n",
      "{\n",
      "    b'118119': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.02444865]], dtype=float32)>,\n",
      "    b'126118': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00833372]], dtype=float32)>,\n",
      "    b'166712': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.0051669]], dtype=float32)>,\n",
      "    b'186470': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.00497086]], dtype=float32)>,\n",
      "    b'219596': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.0289848]], dtype=float32)>,\n",
      "    b'228179': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00571151]], dtype=float32)>,\n",
      "    b'298748': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00461718]], dtype=float32)>,\n",
      "    b'435013': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.00280774]], dtype=float32)>,\n",
      "    b'44551': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00873445]], dtype=float32)>,\n",
      "    b'82783': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00636458]], dtype=float32)>,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "user_rand = random.choice(uniqueUserIds)\n",
    "print(user_rand)\n",
    "test_rating = {}\n",
    "for m in test_data.take(10):\n",
    "    test_rating[m[\"foodID\"].numpy()]=RankingModel()(tf.convert_to_tensor([user_rand]),tf.convert_to_tensor([m[\"foodID\"]])) # type: ignore\n",
    "print(\"test_rating\")\n",
    "pprintpp.pprint(test_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommended products for User 408066: \n",
      "        recipe_id  mean  count  score\n",
      "170501     100419   5.0      1    4.0\n",
      "        recipe_id  mean  count  score\n",
      "188288     279963   5.0      1    4.0\n",
      "        recipe_id  mean  count  score\n",
      "205526     243247   5.0      1    4.0\n",
      "       recipe_id  mean  count    score\n",
      "26216       9166   4.0      8  4.80618\n",
      "        recipe_id  mean  count  score\n",
      "228424     315173   5.0      1    4.0\n",
      "        recipe_id  mean  count    score\n",
      "121728     339387   4.5      2  4.10206\n",
      "       recipe_id  mean  count     score\n",
      "11773       9425   4.4     15  5.752183\n",
      "        recipe_id  mean  count  score\n",
      "200904     207470   5.0      1    4.0\n",
      "        recipe_id  mean  count  score\n",
      "156275     364237   4.0      1    3.0\n",
      "       recipe_id   mean  count    score\n",
      "28415     272643  4.125      8  4.93118\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 recommended products for User {}: \".format(user_rand))\n",
    "for m in sorted(test_rating, key=test_rating.get, reverse=True): # type: ignore\n",
    "    recipe = df4.loc[recipe_data['id'] == int(m.decode())]\n",
    "    recipe = recipe[['recipe_id', 'mean', 'count', \"score\"]]\n",
    "    print(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to serialize <StringArray>\n[     '38094',    '1293707',       '8937',     '126440',      '57222',\n      '52282',     '124416', '2000192946',      '76535',     '273745',\n ...\n     '157255', '2002300998', '2002212283', '2000497761', '2000145340',\n '2001868099',    '1197076',    '2405600', '2000127684',     '116593']\nLength: 226570, dtype: string to JSON. Unrecognized type <class 'pandas.core.arrays.string_.StringArray'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# build and save model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 48\u001b[0m, in \u001b[0;36mFoodModel.save_model\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mranking_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\json\\encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\json\\encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to serialize <StringArray>\n[     '38094',    '1293707',       '8937',     '126440',      '57222',\n      '52282',     '124416', '2000192946',      '76535',     '273745',\n ...\n     '157255', '2002300998', '2002212283', '2000497761', '2000145340',\n '2001868099',    '1197076',    '2405600', '2000127684',     '116593']\nLength: 226570, dtype: string to JSON. Unrecognized type <class 'pandas.core.arrays.string_.StringArray'>."
     ]
    }
   ],
   "source": [
    "# build and save model\n",
    "model.save_model(\"./model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
